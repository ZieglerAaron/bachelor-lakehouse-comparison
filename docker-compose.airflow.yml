version: '3.8'
services:
  airflow:
    image: apache/airflow:2.8.1-python3.10
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=UwynEqzpKSkMNiMiuaIAHV3Z5GHZ6GmTMdqXNUlIJhQ=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=airflowsecretkeyairflowsecretkey
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__WEBSERVER__PORT=8080
      - AIRFLOW_CONN_TRINO_ICEBERG=trino://controller:8080?catalog=iceberg&schema=default
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/requirements.txt:/requirements.txt
      - ./delta/spark-apps:/opt/spark-apps
      - ./delta/spark-extra-jars:/opt/spark/extra-jars
    depends_on: []
    command: >
      bash -c "
        pip install --no-cache-dir -r /requirements.txt &&
        airflow standalone
      "
    networks:
      - lakehouse
networks:
  lakehouse:
    external: true 